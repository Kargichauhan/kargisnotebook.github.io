<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Neurosymbolic AI: When Logic Meets Learning</title>
  <link rel="icon" href="../favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" href="../style.css" />
  <link href="https://fonts.googleapis.com/css2?family=Outfit:wght@400;600&display=swap" rel="stylesheet" />
</head>

<body>
  <h1>ðŸ§  Neurosymbolic AI: When Logic Meets Learning</h1>
  <p class="meta">Date: May 1, 2025 â€¢ Estimated Reading Time: 10 min â€¢ Author: Kargi Chauhan</p>

  <details class="toc">
    <summary>ðŸ“š Table of Contents</summary>
    <ul>
      <li><a href="#what-is">What is Neurosymbolic AI?</a></li>
      <li><a href="#why-it-matters">Why It Matters</a></li>
      <li><a href="#ltn">Example: Logic Tensor Networks</a></li>
      <li><a href="#future">Challenges and Future</a></li>
      <li><a href="#citation">Citation</a></li>
      <li><a href="#references">References</a></li>
    </ul>
  </details>

  <h2 id="what-is">What is Neurosymbolic AI?</h2>
  <p>
    Neurosymbolic AI is a hybrid approach that combines symbolic reasoning (like logic rules and ontologies) with neural network-based learning (like transformers or CNNs). 
    It aims to bring together the strengths of <strong>structure</strong> and <strong>data-driven generalization</strong>.
  </p>

  <h2 id="why-it-matters">Why It Matters</h2>
  <p>
    Purely neural models lack <strong>explainability</strong>, and symbolic systems lack <strong>flexibility</strong>. Neurosymbolic systems can reason over facts and learn from data â€” bridging the gap between <code>deduction</code> and <code>pattern recognition</code>.
  </p>

  <h2 id="ltn">Example: Logic Tensor Networks</h2>
  <p>
    LTNs integrate first-order logic into deep learning by relaxing logical operators using continuous semantics. For instance, you can encode:
  </p>
  <pre><code>âˆ€x: Human(x) â†’ Mortal(x)</code></pre>
  <p>
    and train your model to learn embeddings that <strong>satisfy these constraints</strong> while fitting the data.
  </p>

  <h2 id="future">Challenges and Future</h2>
  <p>
    While promising, neurosymbolic AI is still under research. Open questions include:
  </p>
  <ul>
    <li>How to scale logic reasoning to real-world datasets?</li>
    <li>How to balance interpretability and model capacity?</li>
    <li>Can LLMs be guided by symbolic priors?</li>
  </ul>

  <h2 id="citation">Citation</h2>
  <p>Cite this blog post as:</p>
  <p><strong>Kargi Chauhan.</strong> (May 2025). <em>Neurosymbolic AI: When Logic Meets Learning</em>. KC's Log.  
    https://kargichauhan.github.io/kargisnotebook.github.io/posts/neurosymbolic-ai.html</p>

  <p><strong>Or in BibTeX:</strong></p>
  <div class="cite-box">
@article{kargi2025neurosymbolic,
  title   = "Neurosymbolic AI: When Logic Meets Learning",
  author  = "Kargi Chauhan",
  journal = "kargisnotebook.github.io",
  year    = "2025",
  month   = "May",
  url     = "https://kargichauhan.github.io/kargisnotebook.github.io/posts/neurosymbolic-ai.html"
}
  </div>

  <h2 id="references">References</h2>
  <ol class="refs">
    <li>Garcez, A. d. et al. "Neurosymbolic AI: The Third Wave." <em>Communications of the ACM</em> (2020)</li>
    <li>Serafini, L., & Garcez, A. "Logic Tensor Networks: Deep Learning and Logical Reasoning." <em>arXiv:1606.04422</em></li>
    <li>Besold et al. "Neural-Symbolic Learning and Reasoning." <em>Springer</em> (2017)</li>
    <li>Marcus, G. "The Next Decade in AI: Why Common Sense Matters." <em>arXiv:2002.06177</em></li>
  </ol>

  <!-- Theme toggle logic -->
  <script src="../theme.js"></script>
</body>
</html>
